@article{grabowski_google_2012,
	title = {Google Books Ngrams Recompressed and Searchable},
	volume = {37},
	issn = {2300-3405, 0867-6356},
	url = {https://content.sciendo.com/doi/10.2478/v10209-011-0015-8},
	doi = {10.2478/v10209-011-0015-8},
	abstract = {One of the research fields significantly affected by the emergence of “big data” is computational linguistics. A prominent example of a large dataset targeting this domain is the collection of Google Books Ngrams, made freely available, for several languages, in July 2009. There are two problems with Google Books Ngrams; the textual format (compressed with Deflate) in which they are distributed is highly inefficient; we are not aware of any tool facilitating search over those data, apart from the Google viewer, which, as a Web tool, has seriously limited use. In this paper we present a simple preprocessing scheme for Google Books Ngrams, enabling also search for an arbitrary n-gram (i.e., its associated statistics) in average time below 0.2 ms. The obtained compression ratio, with Deflate (zip) left as the backend coder, is over 3 times higher than in the original distribution.},
	pages = {271--281},
	number = {4},
	journaltitle = {Foundations of Computing and Decision Sciences},
	author = {Grabowski, Szymon and Swacha, Jakub},
	urldate = {2020-10-05},
	date = {2012-12-01},
	langid = {english},
	file = {Grabowski and Swacha - 2012 - Google Books Ngrams Recompressed and Searchable.pdf:/Users/casparfriedrich/Zotero/storage/GE5JAHJG/Grabowski and Swacha - 2012 - Google Books Ngrams Recompressed and Searchable.pdf:application/pdf}
}
@inproceedings{banko_scaling_2001,
	location = {Toulouse, France},
	title = {Scaling to very very large corpora for natural language disambiguation},
	url = {http://portal.acm.org/citation.cfm?doid=1073012.1073017},
	doi = {10.3115/1073012.1073017},
	abstract = {The amount of readily available online text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.},
	eventtitle = {the 39th Annual Meeting},
	pages = {26--33},
	booktitle = {Proceedings of the 39th Annual Meeting on Association for Computational Linguistics  - {ACL} '01},
	publisher = {Association for Computational Linguistics},
	author = {Banko, Michele and Brill, Eric},
	urldate = {2020-10-13},
	date = {2001},
	langid = {english},
	file = {Banko and Brill - 2001 - Scaling to very very large corpora for natural lan.pdf:/Users/casparfriedrich/Zotero/storage/ET97C5ZZ/Banko and Brill - 2001 - Scaling to very very large corpora for natural lan.pdf:application/pdf}
}
@inproceedings{numba, author = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley}, title = {Numba: A LLVM-Based Python JIT Compiler}, year = {2015}, isbn = {9781450340052}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2833157.2833162}, doi = {10.1145/2833157.2833162}, abstract = {Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is often a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addition, we share our experience in building a JIT compiler using LLVM[1].}, booktitle = {Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC}, articleno = {7}, numpages = {6}, keywords = {Python, LLVM, compiler}, location = {Austin, Texas}, series = {LLVM '15} }

@article{sklearn,
  author  = {Fabian Pedregosa and Ga{{\"e}}l Varoquaux and Alexandre Gramfort and Vincent Michel and Bertrand Thirion and Olivier Grisel and Mathieu Blondel and Peter Prettenhofer and Ron Weiss and Vincent Dubourg and Jake Vanderplas and Alexandre Passos and David Cournapeau and Matthieu Brucher and Matthieu Perrot and {{\'E}}douard Duchesnay},
  title   = {Scikit-learn: Machine Learning in Python},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {85},
  pages   = {2825-2830},
  url     = {http://jmlr.org/papers/v12/pedregosa11a.html}
}
@article{numpy,
abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
author = {Harris, Charles R and Millman, K Jarrod and van der Walt, St{\'{e}}fan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H and Brett, Matthew and Haldane, Allan and del R{\'{i}}o, Jaime Fern{\'{a}}ndez and Wiebe, Mark and Peterson, Pearu and G{\'{e}}rard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E},
doi = {10.1038/s41586-020-2649-2},
issn = {1476-4687},
journal = {Nature},
number = {7825},
pages = {357--362},
title = {{Array programming with NumPy}},
url = {https://doi.org/10.1038/s41586-020-2649-2},
volume = {585},
year = {2020}
}

@article{hardsoft,
   Author="Storer, N. W. ",
   Title="{{T}he hard sciences and the soft: some sociological observations}",
   Journal="Bull Med Libr Assoc",
   Year="1967",
   Volume="55",
   Number="1",
   Pages="75--84",
   Month="Jan"
}

@article{mapreduce,
	title = {{MapReduce}: simplified data processing on large clusters},
	volume = {51},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/1327452.1327492},
	doi = {10.1145/1327452.1327492},
	shorttitle = {{MapReduce}},
	abstract = {{MapReduce} is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper.},
	pages = {107--113},
	number = {1},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Dean, Jeffrey and Ghemawat, Sanjay},
	urldate = {2020-08-10},
	date = {2008-01},
	langid = {english},
	file = {Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf:/Users/casparfriedrich/Zotero/storage/ZS5FSTPD/Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf:application/pdf}
}

@online{WikiCorpus,
    Author={Javier Artiles and Satoshi Sekine},
    Title={Tagged and Cleaned Wikipedia (TC Wikipedia) and its Ngram},
    Year={2008},
    Url={https://nlp.cs.nyu.edu/wikipedia-data/},
    UrlDate={2020-08-04}
}
@inproceedings{seymore1996,
	location = {Philadelphia, {PA}, {USA}},
	title = {Scalable backoff language models},
	volume = {1},
	isbn = {978-0-7803-3555-4},
	url = {http://ieeexplore.ieee.org/document/607084/},
	doi = {10.1109/ICSLP.1996.607084},
	eventtitle = {Proceeding of Fourth International Conference on Spoken Language Processing. {ICSLP} '96},
	pages = {232--235},
	publisher = {{IEEE}},
	author = {Seymore, K. and Rosenfeld, R.},
	urldate = {2020-08-08},
	date = {1996},
	langid = {english},
	file = {Seymore and Rosenfeld - 1996 - Scalable backoff language models.pdf:/Users/casparfriedrich/Zotero/storage/SAN9J5GM/Seymore and Rosenfeld - 1996 - Scalable backoff language models.pdf:application/pdf}
}
@online{NgramViewer,
    Title={Google Books Ngram Viewer},
    Year={2011},
    Url={https://books.google.com/ngrams/info},
    UrlDate={2020-08-04}
}
@article{Brown2020,
    title={Language Models are Few-Shot Learners},
    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year={2020},
    eprint={2005.14165},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@book {SpeechAndLanguageProcessing,
Author={Dan Jurafsky and James H. Martin},
Title={Speech and Language Processing (3rd ed. draft)},
Year={{2019}},
Month={{OCT 16}}
}
@article{Bengio2003,
Author = {Bengio, Y and Ducharme, R and Vincent, P and Jauvin, C},
Title = {{A neural probabilistic language model}},
Journal = {{JOURNAL OF MACHINE LEARNING RESEARCH}},
Year = {{2003}},
Volume = {{3}},
Number = {{6}},
Pages = {{1137-1155}},
Month = {{AUG 15}},
Note = {{Workshop on Machine Learning Methods for Text and Images, VANCOUVER,
   CANADA, 2001}},
Abstract = {{A goal of statistical language modeling is to learn the joint
   probability function of sequences of words in a language. This is
   intrinsically difficult because of the curse of dimensionality: a word
   sequence on which the model will be tested is likely to be different
   from all the word sequences seen during training. Traditional but very
   successful approaches based on n-grams obtain generalization by
   concatenating very short overlapping sequences seen in the training set.
   We propose to fight the curse of dimensionality by learning a
   distributed representation for words which allows each training sentence
   to inform the model about an exponential number of semantically
   neighboring sentences. The model learns simultaneously (1) a distributed
   representation for each word along with (2) the probability function for
   word sequences, expressed in terms of these representations.
   Generalization is obtained because a sequence of words that has never
   been seen before gets high probability if it is made of words that are
   similar (in the sense of having a nearby representation) to words
   forming an already seen sentence. Training such large models (with
   millions of parameters) within a reasonable time is itself a significant
   challenge. We report on experiments using neural networks for the
   probability function, showing on two text corpora that the proposed
   approach significantly improves on state-of-the-art n-gram models, and
   that the proposed approach allows to take advantage of longer contexts.}},
Publisher = {{MICROTOME PUBL}},
Address = {{31 GIBBS ST, BROOKLINE, MA 02446 USA}},
Type = {{Article; Proceedings Paper}},
Language = {{English}},
Affiliation = {{Bengio, Y (Corresponding Author), Univ Montreal, Ctr Rech Math, Dept Informat \& Rech Operat, Montreal, PQ H3C 3J7, Canada.
   Univ Montreal, Ctr Rech Math, Dept Informat \& Rech Operat, Montreal, PQ H3C 3J7, Canada.}},
DOI = {{10.1162/153244303322533223}},
ISSN = {{1532-4435}},
Keywords = {{statistical language modeling; artificial neural networks; distributed
   representation; curse of dimensionality}},
Keywords-Plus = {{NETWORKS}},
Research-Areas = {{Automation \& Control Systems; Computer Science}},
Web-of-Science-Categories  = {{Automation \& Control Systems; Computer Science, Artificial Intelligence}},
Author-Email = {{BENGIOY@IRO.UMONTREAL.CA
   DUCHARME@IRO.UMONTREAL.CA
   VINCENTP@IRO.UMONTREAL.CA
   JAUVINC@IRO.UMONTREAL.CA}},
Number-of-Cited-References = {{33}},
Times-Cited = {{1845}},
Usage-Count-Last-180-days = {{20}},
Usage-Count-Since-2013 = {{208}},
Journal-ISO = {{J. Mach. Learn. Res.}},
Doc-Delivery-Number = {{733LT}},
Unique-ID = {{ISI:000186002400006}},
DA = {{2020-08-03}},
}

@article{Bloom1970,
	title = {Space/time trade-offs in hash coding with allowable errors},
	volume = {13},
	issn = {00010782},
	url = {http://portal.acm.org/citation.cfm?doid=362686.362692},
	doi = {10.1145/362686.362692},
	abstract = {In this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hashcoding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency. The new methods are intended to reduce the amount of space required to contain the hash-coded information from that associated with conventional methods. The reduction in space is accomplished by exploiting the possibility that a small fraction of errors of commission may be tolerable in some applications, in particular, applications in which a large amount of data is involved and a core resident hash area is consequently not feasible using conventional methods. In such applications, it is envisaged that overall performance could be improved by using a smaller core resident hash area in conjunction with the new methods and, when necessary, by using some secondary and perhaps time-consuming test to "catch" the small fraction of errors associated with the new methods. An example is discussed which illustrates possible areas of application for the new methods. Analysis of the paradigm problem demonstrates that allowing a small number of test messages to be falsely identified as members of the given set will permit a much smaller hash area to be used without increasing reject time.},
	pages = {422--426},
	number = {7},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Bloom, Burton H.},
	urldate = {2020-07-01},
	date = {1970-07-01},
	langid = {english},
	file = {Bloom - 1970 - Spacetime trade-offs in hash coding with allowabl.pdf:/Users/casparfriedrich/Zotero/storage/8LZT99U8/Bloom - 1970 - Spacetime trade-offs in hash coding with allowabl.pdf:application/pdf}
}

@article{katz1987,
	title = {Estimation of probabilities from sparse data for the language model component of a speech recognizer},
	volume = {35},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1165125/},
	doi = {10.1109/TASSP.1987.1165125},
	abstract = {The description of a novel typeof rn-gram language model is given. The modeloffers, via a nonlinear recursive procedure, a computation and space efficient solutionto the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the {IBM} Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises.},
	pages = {400--401},
	number = {3},
	journaltitle = {{IEEE} Transactions on Acoustics, Speech, and Signal Processing},
	shortjournal = {{IEEE} Trans. Acoust., Speech, Signal Process.},
	author = {Katz, S.},
	urldate = {2020-07-01},
	date = {1987-03},
	langid = {english},
	file = {Katz - 1987 - Estimation of probabilities from sparse data for t.pdf:/Users/casparfriedrich/Zotero/storage/K4W9PVW6/Katz - 1987 - Estimation of probabilities from sparse data for t.pdf:application/pdf}
}

@inproceedings{Stolcke2000,
	title = {Entropy-based Pruning of Backoff Language Models},
	booktitle= {DARPA Broadcast News Transcription and Understanding Workshop},
	author = {Andreas Stolcke},
    pages={270-274},
	date = {1998},
	langid = {english},
}

@inproceedings{Talbot2007,
  title={Smoothed Bloom Filter Language Models: Tera-Scale LMs on the Cheap},
  author={David Talbot and M. Osborne},
  booktitle={EMNLP-CoNLL},
  year={2007}
}

@inproceedings{Church2007,
  title={Compressing trigram language models with Golomb coding},
  author={Church, Kenneth and Hart, Ted and Gao, Jianfeng},
  eventtitle={Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)},
  pages={199--207},
  year={2007}
}

@article{Fischler1980,
  title={Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography},
  author={Fischler, Martin A and Bolles, Robert C},
  journal={Communications of the ACM},
  volume={24},
  number={6},
  pages={381--395},
  year={1981},
  publisher={ACM New York, NY, USA}
}

@inproceedings{kiss2018,
	location = {Honolulu, {HI}},
	title = {Bloom Filter with a False Positive Free Zone},
	isbn = {978-1-5386-4128-6},
	url = {https://ieeexplore.ieee.org/document/8486415/},
	doi = {10.1109/INFOCOM.2018.8486415},
	abstract = {Bloom ﬁlters and their variants are widely used as space efﬁcient probabilistic data structures for representing set systems and are very popular in networking applications. They support fast element insertion and deletion, along with membership queries with the drawback of false positives. Bloom ﬁlters can be designed to match the false positive rates that are acceptable for the application domain. However, in many applications a common engineering solution is to set the false positive rate very small, and ignore the existence of the very unlikely false positive answers. This paper is devoted to close the gap between the two design concepts of unlikely and not having false positives. We propose a data structure, called {EGH} ﬁlter, that supports the Bloom ﬁlter operations and besides it can guarantee false positive free operations for a ﬁnite universe and a restricted number of elements stored in the ﬁlter. We refer to the limited universe and ﬁlter size as the false positive free zone of the ﬁlter. We describe necessary conditions for the false positive free zone of a ﬁlter and generalize the ﬁlter to support listing of the elements. We evaluate the performance of the ﬁlter in comparison with the traditional Bloom ﬁlters. Our data structure is based on recently developed combinatorial group testing techniques.},
	eventtitle = {{IEEE} {INFOCOM} 2018 - {IEEE} Conference on Computer Communications},
	pages = {1412--1420},
	booktitle = {{IEEE} {INFOCOM} 2018 - {IEEE} Conference on Computer Communications},
	publisher = {{IEEE}},
	author = {Kiss, Sandor Z. and Hosszu, Eva and Tapolcai, Janos and Ronyai, Lajos and Rottenstreich, Ori},
	urldate = {2020-07-07},
	date = {2018-04},
	langid = {english},
	file = {Kiss et al. - 2018 - Bloom Filter with a False Positive Free Zone.pdf:/Users/casparfriedrich/Zotero/storage/2ZUYVLIR/Kiss et al. - 2018 - Bloom Filter with a False Positive Free Zone.pdf:application/pdf}
}

@article{Michel2011,
	title = {Quantitative Analysis of Culture Using Millions of Digitized Books},
	volume = {331},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.1199644},
	doi = {10.1126/science.1199644},
	pages = {176--182},
	number = {6014},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Michel, J.-B. and Shen, Y. K. and Aiden, A. P. and Veres, A. and Gray, M. K. and {The Google Books Team} and Pickett, J. P. and Hoiberg, D. and Clancy, D. and Norvig, P. and Orwant, J. and Pinker, S. and Nowak, M. A. and Aiden, E. L.},
	urldate = {2020-07-08},
	date = {2011-01-14},
	langid = {english},
	file = {Michel et al. - 2011 - Quantitative Analysis of Culture Using Millions of.pdf:/Users/casparfriedrich/Zotero/storage/RQTW5MIK/Michel et al. - 2011 - Quantitative Analysis of Culture Using Millions of.pdf:application/pdf}
}

@misc{Mikolov2012,
	title = {Statistical Language Models Based on Neural Networks},
	author = {Mikolov},
	date = {2012},
	file = {final-thesis.pdf:/Users/casparfriedrich/Zotero/storage/VJHNGJ4A/final-thesis.pdf:application/pdf}
}

@article{pedregosa_scikit-learn_nodate,
	title = {Scikit-learn: Machine Learning in Python},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and {API} consistency. It has minimal dependencies and is distributed under the simpliﬁed {BSD} license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	pages = {6},
	journaltitle = {{MACHINE} {LEARNING} {IN} {PYTHON}},
	author = {Pedregosa, Fabian and Varoquaux, Gael and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David},
	langid = {english},
	file = {Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf:/Users/casparfriedrich/Zotero/storage/9H2I2PK3/Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf:application/pdf}
}

@article{Broder2004,
	title = {Network Applications of Bloom Filters: A Survey},
	volume = {1},
	issn = {1542-7951, 1944-9488},
	url = {http://www.internetmathematicsjournal.com/article/1393},
	doi = {10.1080/15427951.2004.10129096},
	shorttitle = {Network Applications of Bloom Filters},
	abstract = {A Bloom ﬁlter is a simple space-eﬃcient randomized data structure for representing a set in order to support membership queries. Bloom ﬁlters allow false positives but the space savings often outweigh this drawback when the probability of an error is controlled. Bloom ﬁlters have been used in database applications since the 1970s, but only in recent years have they become popular in the networking literature. The aim of this paper is to survey the ways in which Bloom ﬁlters have been used and modiﬁed in a variety of network problems, with the aim of providing a uniﬁed mathematical and practical framework for understanding them and stimulating their use in future applications.},
	pages = {485--509},
	number = {4},
	journaltitle = {Internet Mathematics},
	shortjournal = {Internet Mathematics},
	author = {Broder, Andrei and Mitzenmacher, Michael},
	urldate = {2020-08-02},
	date = {2004-01},
	langid = {english},
	file = {Broder and Mitzenmacher - 2004 - Network Applications of Bloom Filters A Survey.pdf:/Users/casparfriedrich/Zotero/storage/BHD2SATN/Broder and Mitzenmacher - 2004 - Network Applications of Bloom Filters A Survey.pdf:application/pdf}
}

